# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import glob
from model import ScalpingLSTM
from config import DEVICE

# [ì„¤ì •]
SEQ_LEN = 10     # 10ê°œë¥¼ ë³´ê³ 
PREDICT_LEN = 1  # 1ê°œë¥¼ ì˜ˆì¸¡
BATCH_SIZE = 32  # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì‚´ì§ ì¦ê°€
EPOCHS = 100     # í•™ìŠµ íšŸìˆ˜ ì¦ê°€

def add_advanced_features(df):
    """
    [Feature Engineering] 
    AIê°€ ì‹œìž¥ì„ ë” ìž˜ ì´í•´í•˜ë„ë¡ ë³´ì¡°ì§€í‘œ 5ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    ì´ 10ê°œ í”¼ì³: [ì¢…ê°€, ì‹œê°€, ê³ ê°€, ì €ê°€, ê±°ëž˜ëŸ‰] + [ì´ê²©ë„5, ì´ê²©ë„20, RSI, ë³€ë™ì„±, ê±°ëž˜ëŸ‰ë³€í™”]
    """
    df = df.copy()
    
    # 0. ê¸°ë³¸ ì „ì²˜ë¦¬ (ìˆ«ìž ë³€í™˜)
    cols = ['stck_prpr', 'stck_oprc', 'stck_hgpr', 'stck_lwpr', 'cntg_vol']
    for col in cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # [ì¤‘ìš”] ì‹œê°„ ìˆœì„œ ì •ë ¬ (ê³¼ê±° -> ë¯¸ëž˜)
    # API ë°ì´í„°ëŠ” ë³´í†µ ì—­ìˆœ(ìµœì‹ ì´ ìœ„)ì´ë¯€ë¡œ ë’¤ì§‘ì–´ì¤˜ì•¼ í•¨
    df = df.iloc[::-1].reset_index(drop=True)

    # 1. ì´ë™í‰ê·  ì´ê²©ë„ (Disparity)
    # ê°€ê²©ì´ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë†’ëƒ/ë‚®ëƒ (1.05 = 5% ë¹„ìŒˆ)
    df['ma5'] = df['stck_prpr'].rolling(window=5).mean()
    df['ma20'] = df['stck_prpr'].rolling(window=20).mean()
    df['disp5'] = df['stck_prpr'] / (df['ma5'] + 1e-8)
    df['disp20'] = df['stck_prpr'] / (df['ma20'] + 1e-8)

    # 2. RSI (ìƒëŒ€ê°•ë„ì§€ìˆ˜)
    delta = df['stck_prpr'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / (loss + 1e-8)
    df['rsi'] = 100 - (100 / (1 + rs))

    # 3. ë¡œê·¸ ìˆ˜ìµë¥  (ë³€ë™ì„±)
    df['log_ret'] = np.log(df['stck_prpr'] / df['stck_prpr'].shift(1))

    # 4. ê±°ëž˜ëŸ‰ ë³€í™”ìœ¨
    df['vol_chg'] = df['cntg_vol'] / (df['cntg_vol'].shift(1) + 1e-8)

    # NaN ì œê±° (ì§€í‘œ ê³„ì‚°í•˜ëŠë¼ ì•žë¶€ë¶„ 20ê°œ ì •ë„ ë¹”)
    df = df.dropna().reset_index(drop=True)
    
    return df

class StockDataset(Dataset):
    def __init__(self, file_paths, seq_len=SEQ_LEN):
        self.samples = []
        
        print(f"ðŸ“‚ í•™ìŠµ ë°ì´í„° ë¡œë”© ë° í”¼ì³ ìƒì„± ì¤‘... (íŒŒì¼ {len(file_paths)}ê°œ)")
        
        for path in file_paths:
            try:
                raw_df = pd.read_csv(path)
                if len(raw_df) < 30: continue # ë°ì´í„° ë„ˆë¬´ ì ìœ¼ë©´ íŒ¨ìŠ¤

                # â˜… ë³´ì¡°ì§€í‘œ ì¶”ê°€ (Feature Engineering)
                df = add_advanced_features(raw_df)
                
                if len(df) < seq_len + 1: continue

                # ì‚¬ìš©í•  ì»¬ëŸ¼ 10ê°œ ì„ ì •
                features = ['stck_prpr', 'stck_oprc', 'stck_hgpr', 'stck_lwpr', 'cntg_vol', 
                            'disp5', 'disp20', 'rsi', 'log_ret', 'vol_chg']
                
                data = df[features].values
                
                # ì •ê·œí™” (MinMax Scaling 0~1)
                # ê° ì»¬ëŸ¼ë³„ë¡œ ìµœëŒ€/ìµœì†Œ êµ¬í•´ì„œ ì •ê·œí™”
                min_vals = data.min(axis=0)
                max_vals = data.max(axis=0)
                
                # ë¶„ëª¨ 0 ë°©ì§€
                ranges = max_vals - min_vals
                ranges[ranges == 0] = 1e-8
                
                scaled_data = (data - min_vals) / ranges

                # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
                for i in range(len(scaled_data) - seq_len):
                    x = scaled_data[i : i+seq_len]      # 10ì¼ì¹˜ ë°ì´í„° (10ê°œ ì»¬ëŸ¼)
                    # ì˜ˆì¸¡ ëª©í‘œ: ë‹¤ìŒë‚ ì˜ 'ì¢…ê°€(Close)' (0ë²ˆì§¸ ì»¬ëŸ¼)
                    y = scaled_data[i+seq_len][0]       
                    
                    self.samples.append((
                        torch.FloatTensor(x), 
                        torch.FloatTensor([y])
                    ))
                    
            except Exception as e:
                # print(f"âš ï¸ ì—ëŸ¬({path}): {e}")
                continue

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

def train():
    print(f"ðŸ”¥ í•™ìŠµ ì‹œìž‘ (Device: {DEVICE})")
    
    file_list = glob.glob("data/raw/*.csv")
    if not file_list:
        print("âŒ 'data/raw' í´ë”ì— CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    dataset = StockDataset(file_list)
    if len(dataset) == 0:
        print("âš ï¸ í•™ìŠµ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    print(f"âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ! (ì´ ìƒ˜í”Œ: {len(dataset)}ê°œ)")

    # [ëª¨ë¸ ìƒì„±] input_size=10 (í”¼ì³ ê°œìˆ˜)
    model = ScalpingLSTM(input_size=10, hidden_size=64, num_layers=2, output_size=1, dropout=0.2).to(DEVICE)
    
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    model.train()
    
    for epoch in range(EPOCHS):
        total_loss = 0
        for x, y in dataloader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        if (epoch+1) % 10 == 0:
            avg_loss = total_loss / len(dataloader)
            print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.6f}")

    torch.save(model.state_dict(), "scalping_model.pth")
    print("ðŸŽ‰ í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ìž¥ë¨: scalping_model.pth")

if __name__ == "__main__":
    train()